// Base LLM Service class
class LLMService {
  async generateReply(comment, style, userPrompt, maxLength = 140) {
    throw new Error('Method not implemented');
  }
}

// Mock implementation for testing
class MockLLMService extends LLMService {
  async generateReply(comment, style, userPrompt, maxLength = 140) {
    // Simulate API delay
    await new Promise(resolve => setTimeout(resolve, 1000));

    // Check if comment is in Chinese (contains Chinese characters)
    const isChinese = /[\u4e00-\u9fff]/.test(comment);

    // Mock responses based on style and language
    const responses = {
      positive: {
        en: "Thanks for sharing your perspective! Really appreciate your thoughtful comment. ğŸ‘",
        zh: "æ„Ÿè°¢åˆ†äº«ä½ çš„è§‚ç‚¹ï¼çœŸçš„å¾ˆæ¬£èµä½ çš„è§è§£ã€‚ ğŸ‘"
      },
      constructive: {
        en: "Interesting point! Have you also considered looking at it from [alternative perspective]? ğŸ¤”",
        zh: "æœ‰è¶£çš„è§‚ç‚¹ï¼ä½ æœ‰æ²¡æœ‰è€ƒè™‘ä»[å…¶ä»–è§’åº¦]æ¥çœ‹è¿™ä¸ªé—®é¢˜ï¼ŸğŸ¤”"
      },
      critical: {
        en: "While I see your point, I respectfully disagree because [reason]. Let's discuss further.",
        zh: "è™½ç„¶æˆ‘ç†è§£ä½ çš„è§‚ç‚¹ï¼Œä½†æˆ‘æŒä¸åŒæ„è§ï¼Œå› ä¸º[åŸå› ]ã€‚æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥è®¨è®ºã€‚"
      }
    };

    // Return appropriate response based on language and style
    return responses[style][isChinese ? 'zh' : 'en'];
  }
}

// OpenAI implementation (to be used later)
class OpenAIService extends LLMService {
  constructor(apiKey) {
    super();
    this.apiKey = apiKey;
  }

  async generateReply(comment, style, userPrompt, maxLength = 140) {
    // TODO: Implement real OpenAI API call
    throw new Error('OpenAI implementation not ready yet');
  }
}

// Factory for creating LLM services
class LLMServiceFactory {
  static createService(type, config) {
    switch (type) {
      case 'mock':
        return new MockLLMService();
      case 'openai':
        return new OpenAIService(config?.apiKey);
      default:
        throw new Error(`Unknown LLM service type: ${type}`);
    }
  }
}

export { LLMService, LLMServiceFactory };